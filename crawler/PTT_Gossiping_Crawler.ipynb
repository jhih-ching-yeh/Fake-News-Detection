{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PTT Gossiping Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdYADo9TNIwT"
      },
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import jieba as jb\n",
        "from bs4 import BeautifulSoup\n",
        "import bs4"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXJbDXiNRcz"
      },
      "source": [
        "def get_ppt_page(url):\n",
        "    # 紀錄cookies 是否年滿18歲\n",
        "    resp = requests.get(\n",
        "        url=url,\n",
        "        cookies={'over18': '1'}  \n",
        "    )\n",
        "    if resp.status_code != 200:\n",
        "        print('Invalid url:', resp.url)\n",
        "        return None\n",
        "    else:\n",
        "        return resp.text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(article_url):\n",
        "    # 文章連結\n",
        "    # URL = \"https://www.ptt.cc/bbs/Gossiping/M.1639675838.A.A33.html\"\n",
        "    # 設定Header與Cookie\n",
        "    my_headers = {'cookie': 'over18=1;'}\n",
        "    # 發送get 請求 到 ptt 八卦版\n",
        "    response = requests.get(article_url, headers = my_headers)\n",
        "\n",
        "\n",
        "    #  把網頁程式碼(HTML) 丟入 bs4模組分析\n",
        "    soup = bs4.BeautifulSoup(response.text,\"html.parser\")\n",
        "\n",
        "    ## PTT 上方4個欄位\n",
        "    header = soup.find_all('span','article-meta-value')\n",
        "\n",
        "\n",
        "    ## 查找所有html 元素 抓出內容\n",
        "    main_container = soup.find(id='main-container')\n",
        "    # 把所有文字都抓出來\n",
        "    all_text = main_container.text\n",
        "    # 把整個內容切割透過 \"-- \" 切割成2個陣列\n",
        "    pre_text = all_text.split('--')[0]\n",
        "        \n",
        "    # 把每段文字 根據 '\\n' 切開\n",
        "    texts = pre_text.split('\\n')\n",
        "    # 如果你爬多篇你會發現 \n",
        "    contents = texts[2:]\n",
        "    # 內容\n",
        "    content = '\\n'.join(contents)\n",
        "\n",
        "    # 顯示\n",
        "    #print('內容: ', content)\n",
        "    return content"
      ],
      "metadata": {
        "id": "PHGm-Hulpurf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IMLPSmNNXFQ"
      },
      "source": [
        "def get_pageinfo(num, resdata, domain_url, pptdata, date_range):\n",
        "    soup = BeautifulSoup(resdata, 'html5lib')\n",
        "    # 取得上一頁按鈕\n",
        "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
        "    prev_url = paging_div.find_all('a')[1]['href']\n",
        "    \n",
        "    # 儲存取得的文章資料<div class=\"r-ent\"></div>\n",
        "    date_divs = soup.find_all('div', 'r-ent')\n",
        "    \n",
        "    whether_prev = True\n",
        "    for k in date_divs:\n",
        "        # 判斷文章是否在一週內發佈\n",
        "        week_date = k.find('div', 'date').text.strip() in date_range\n",
        "        \n",
        "        # 代表最早的日期，就不符合條件，所以不用下一頁\n",
        "        # 因為第一頁涵蓋熱門文章，因此日期並非按照順序，所以從第二頁開始判斷\n",
        "        if num >= 1:\n",
        "            if week_date == False:\n",
        "                whether_prev = False\n",
        "\n",
        "        # 一周內發布       \n",
        "        if week_date:\n",
        "            # 發布日期\n",
        "            post_date = k.find('div', 'date').text.strip()\n",
        "            #print(\"發布日期\",post_date)\n",
        "            \n",
        "            # 推文數\n",
        "            push_count = k.find('div', 'nrec').text\n",
        "            push_num = 0\n",
        "            if push_count:\n",
        "                try:\n",
        "                    push_num = int(push_count)  \n",
        "                except ValueError:\n",
        "                    # 若轉換失敗，可能是'爆'或 'X1', 'X2'\n",
        "                    if push_count == '爆':\n",
        "                        push_num = 100\n",
        "                    elif push_count.startswith('X'):\n",
        "                        push_num = -100\n",
        "                    else:\n",
        "                        push_num = 0\n",
        "            #print(\"推文數\",push_num)\n",
        "            \n",
        "            # 有超連結，代表文章存在\n",
        "            if k.find('a'): \n",
        "                # 文章標題\n",
        "                title = k.find('a').text\n",
        "                #print(\"標題\",title)\n",
        "                # 文章連結\n",
        "                href = k.find('a')['href']\n",
        "                #print(\"標題連結\",domain_url+href)\n",
        "                # 文章內文\n",
        "                content = get_content(domain_url+href)\n",
        "\n",
        "                # 儲存資料\n",
        "                pptdata.append({\n",
        "                    'date': post_date,\n",
        "                    'push_num': push_num,\n",
        "                    'title': title,\n",
        "                    'content': content,\n",
        "                    'href': domain_url+href\n",
        "                })\n",
        "\n",
        "        \n",
        "    \n",
        "    # 每做完一頁，num+1\n",
        "    num += 1\n",
        "    \n",
        "    \n",
        "    return prev_url, pptdata, whether_prev, num"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8PbVe6JNbH_"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # *** 選擇八卦版 ***\n",
        "    domain_url = 'https://www.ptt.cc'\n",
        "    Gossiping = '/bbs/Gossiping/index.html'\n",
        "    today = time.strftime(\"%m/%d\").lstrip('0')\n",
        "    date_range = [today]\n",
        "    pptdata = []  \n",
        "    num = 0\n",
        "\n",
        "    # 第一頁 (含熱門文章)\n",
        "    web_url = domain_url + Gossiping\n",
        "    ppt_page = get_ppt_page(web_url)\n",
        "    prev_href, pptdata, whether_prev, num = get_pageinfo(num, ppt_page, domain_url, pptdata, date_range) \n",
        "    \n",
        "    # 往前一頁\n",
        "    while whether_prev:\n",
        "        web_url = domain_url+prev_href\n",
        "        ppt_page = get_ppt_page(web_url)\n",
        "        prev_href, pptdata, whether_prev, num = get_pageinfo(num, ppt_page, domain_url, pptdata, date_range) \n",
        "    \n",
        "    # 將list轉成dataframe\n",
        "    df = pd.DataFrame(pptdata, columns=['date','push_num', 'title', 'content', 'href'])  \n",
        "\n",
        "    # 存成dataframe\n",
        "      "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_ = date_range[0].replace(\"/\", \"_\")"
      ],
      "metadata": {
        "id": "GxPuw-m43cUq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXTG2EsgNdqL"
      },
      "source": [
        "df.to_csv(date_+\"_ppt_result.csv\", index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RXKAzj_wtM-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}